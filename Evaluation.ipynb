{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/girijesh-ai/llamaIndex-projects/blob/main/Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "Evaluation and benchmarking play a pivotal role in the development of LLM Applications. For optimizing the performance of applications such as RAG (Retrieval Augmented Generation), a robust measurement mechanism is indispensable.\n",
        "\n",
        "LlamaIndex offers vital modules tailored to assess the quality of generated outputs. Additionally, it incorporates specialized modules designed specifically to evaluate content retrieval quality. LlamaIndex categorizes its evaluation into two primary types:\n",
        "\n",
        "*   **Response Evaluation**\n",
        "*   **Retrieval Evaluation**\n",
        "\n",
        "[Documentation\n",
        "](https://gpt-index.readthedocs.io/en/latest/core_modules/supporting_modules/evaluation/root.html)"
      ],
      "metadata": {
        "id": "QYkWSVQDPckV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyxrLlm4V0aZ",
        "outputId": "90e517b6-4d33-447a-c7b8-6628ba3ac0f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.8.53.post3-py3-none-any.whl (794 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m794.6/794.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2.0.22)\n",
            "Collecting aiostream<0.6.0,>=0.5.2 (from llama-index)\n",
            "  Downloading aiostream-0.5.2-py3-none-any.whl (39 kB)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from llama-index)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2023.6.0)\n",
            "Collecting langchain>=0.0.303 (from llama-index)\n",
            "  Downloading langchain-0.0.325-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.8)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.23.5)\n",
            "Collecting openai>=0.26.4 (from llama-index)\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (8.2.3)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index)\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (4.5.0)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting urllib3<2 (from llama-index)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->llama-index)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index) (1.14.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (6.0.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (4.0.3)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain>=0.0.303->llama-index)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.52 (from langchain>=0.0.303->llama-index)\n",
            "  Downloading langsmith-0.0.53-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (2.31.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (4.66.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2023.3.post1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain>=0.0.303->llama-index) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain>=0.0.303->llama-index) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain>=0.0.303->llama-index) (1.1.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain>=0.0.303->llama-index)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->llama-index) (23.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.303->llama-index) (2023.7.22)\n",
            "Installing collected packages: urllib3, mypy-extensions, marshmallow, jsonpointer, deprecated, aiostream, typing-inspect, jsonpatch, tiktoken, openai, langsmith, dataclasses-json, langchain, llama-index\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiostream-0.5.2 dataclasses-json-0.5.14 deprecated-1.2.14 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.325 langsmith-0.0.53 llama-index-0.8.53.post3 marshmallow-3.20.1 mypy-extensions-1.0.0 openai-0.28.1 tiktoken-0.5.1 typing-inspect-0.9.0 urllib3-1.26.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Response Evaluation\n",
        "\n",
        "Evaluating results from LLMs is distinct from traditional machine learning's straightforward outcomes. LlamaIndex employs evaluation modules, using a benchmark LLM like GPT-4, to gauge answer accuracy. Notably, these modules often blend query, context, and response, minimizing the need for ground-truth labels.\n",
        "\n",
        "The evaluation modules manifest in the following categories:\n",
        "\n",
        "*   **Faithfulness:** Assesses whether the response remains true to the retrieved contexts, ensuring there's no distortion or \"hallucination.\"\n",
        "*   **Context Relevancy:** Evaluates the relevance of both the retrieved context and the generated answer to the initial query.\n",
        "*   **Correctness:** Determines if the generated answer aligns with the reference answer based on the query (this does require labels).\n",
        "*   **Guideline Adherence:** Examines whether the predicted answer conforms to specific predefined guidelines.\n",
        "\n",
        "Furthermore, LlamaIndex has the capability to autonomously generate questions from your data, paving the way for an evaluation pipeline to assess the RAG application."
      ],
      "metadata": {
        "id": "oTMyT_qQSH0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# attach to the same event-loop\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "# Set up the root logger\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)  # Set logger level to INFO\n",
        "\n",
        "# Clear out any existing handlers\n",
        "logger.handlers = []\n",
        "\n",
        "# Set up the StreamHandler to output to sys.stdout (Colab's output)\n",
        "handler = logging.StreamHandler(sys.stdout)\n",
        "handler.setLevel(logging.INFO)  # Set handler level to INFO\n",
        "\n",
        "# Add the handler to the logger\n",
        "logger.addHandler(handler)"
      ],
      "metadata": {
        "id": "4fTQJZDiZtIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O384ocD_OjDG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d8a4cda-b60d-4ed9-8955-25ad299b0b67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumExpr defaulting to 2 threads.\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "import sys\n",
        "import pandas as pd\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "from llama_index.evaluation import (\n",
        "    DatasetGenerator,\n",
        "    FaithfulnessEvaluator,\n",
        "    RelevancyEvaluator,\n",
        "    CorrectnessEvaluator,\n",
        "    GuidelineEvaluator,\n",
        "    RetrieverEvaluator,\n",
        "    generate_question_context_pairs,\n",
        "    EmbeddingQAFinetuneDataset\n",
        ")\n",
        "\n",
        "from llama_index import (\n",
        "    SimpleDirectoryReader,\n",
        "    VectorStoreIndex,\n",
        "    ServiceContext,\n",
        "    LLMPredictor,\n",
        "    Response,\n",
        ")\n",
        "\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index.node_parser import SimpleNodeParser\n",
        "\n",
        "import os\n",
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = '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'"
      ],
      "metadata": {
        "id": "djqOM6UfVzhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download Data"
      ],
      "metadata": {
        "id": "CChQ98mgWGcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p 'data/paul_graham/'\n",
        "!wget 'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7smekBCXWS3X",
        "outputId": "e7ca1bf1-8d59-48a0-84d4-4674a9aa0c73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-28 09:32:52--  https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75042 (73K) [text/plain]\n",
            "Saving to: ‘data/paul_graham/paul_graham_essay.txt’\n",
            "\n",
            "\r          data/paul   0%[                    ]       0  --.-KB/s               \rdata/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-10-28 09:32:52 (3.37 MB/s) - ‘data/paul_graham/paul_graham_essay.txt’ saved [75042/75042]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load Data"
      ],
      "metadata": {
        "id": "uNfuJB0xXKw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reader = SimpleDirectoryReader(\"./data/paul_graham/\")\n",
        "documents = reader.load_data()"
      ],
      "metadata": {
        "id": "hIz7x-91VyuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate Question"
      ],
      "metadata": {
        "id": "mVy40TPDXQLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_generator = DatasetGenerator.from_documents(documents)\n",
        "eval_questions = data_generator.generate_questions_from_nodes()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iCdPoP8XMY6",
        "outputId": "b3f5a062-1cfb-40a8-bb5d-1530aabfbf06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chunk_size_limit is deprecated, please specify chunk_size instead\n",
            "chunk_size_limit is deprecated, please specify chunk_size instead\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3069 request_id=5844ee2d051ae232bc779634abbb0e99 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3069 request_id=5844ee2d051ae232bc779634abbb0e99 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3138 request_id=3f787fec87e055fb0e9ea03149b1e1c2 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3138 request_id=3f787fec87e055fb0e9ea03149b1e1c2 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3119 request_id=aa320e02b1b41c341d7a3edc25adb160 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3119 request_id=aa320e02b1b41c341d7a3edc25adb160 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3157 request_id=b5a55d6f18e27d37754a48cd9319aa9b response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3157 request_id=b5a55d6f18e27d37754a48cd9319aa9b response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3694 request_id=fb2aef9fa9487f91c6222362246e354d response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=3694 request_id=fb2aef9fa9487f91c6222362246e354d response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=4390 request_id=c9de0549c02efec2d6266693132c39c4 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=4390 request_id=c9de0549c02efec2d6266693132c39c4 response_code=200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(eval_questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ve76lqYJXWF3",
        "outputId": "d4654c9f-cb06-473e-fdfe-7fc148ffe17f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What were the two main things the author worked on before college?',\n",
              " 'How did the author describe their early attempts at writing short stories?',\n",
              " 'What type of computer did the author first work on for programming?',\n",
              " 'What language did the author use for programming on the IBM 1401?',\n",
              " \"What was the author's experience with programming on the IBM 1401?\",\n",
              " 'What type of computer did the author eventually convince their father to buy?',\n",
              " 'Why did the author decide to switch from studying philosophy to AI in college?',\n",
              " \"What were the two things that influenced the author's interest in AI?\",\n",
              " 'What language did the author focus on during their self-teaching of AI?',\n",
              " 'Why did the author decide to focus on Lisp instead of AI?',\n",
              " 'What was the purpose of the entrance exam for studying art in Florence?',\n",
              " 'How did the author manage to pass the written exam despite limited vocabulary?',\n",
              " 'What was the arrangement between the students and faculty in the painting department at the Accademia?',\n",
              " \"How did the author's experience painting still lives differ from painting people?\",\n",
              " 'Why did the author enjoy painting still lives?',\n",
              " 'What important lesson did the author learn at Interleaf?',\n",
              " 'Why did the author decide to drop out of RISD?',\n",
              " \"What was the author's plan for making money in New York?\",\n",
              " 'Why did the author believe the web would be a big deal?',\n",
              " 'What was the initial startup idea of the author and Robert Morris, and why did it not succeed?',\n",
              " 'What was the initial plan for building online stores in the summer of 1995?',\n",
              " 'How did the idea of running the software on the server and controlling it through the browser come about?',\n",
              " 'What was the name of the new company that was started to build online stores?',\n",
              " 'How did the founders of Viaweb fund their startup initially?',\n",
              " 'What was the main goal of an online store builder according to the author?',\n",
              " 'Why did the author feel it was important to build stores for users?',\n",
              " 'What was the growth rate of Viaweb in its early years?',\n",
              " 'Why did the author feel relieved when Yahoo bought Viaweb?',\n",
              " 'What did the author plan to do after leaving Yahoo?',\n",
              " \"How did the author's life change after becoming rich?\",\n",
              " \"What was the author's initial excitement when walking past charming little restaurants?\",\n",
              " \"What was the author's idea for a web app and how did they plan to implement it?\",\n",
              " 'Why did the author decide to move to Cambridge and start a new company?',\n",
              " 'Who did the author recruit to work on the new company and what were their roles?',\n",
              " 'What was the name of the new company and what type of company was it?',\n",
              " \"Why did the author decide to build a subset of the company's vision as an open source project?\",\n",
              " \"How did the author's essays gain a wider audience and what did this realization mean for publishing?\",\n",
              " 'What was the turning point for the author in realizing what they wanted to work on?',\n",
              " \"How did the author's meeting with Jessica Livingston lead to the creation of a new investment firm?\",\n",
              " \"What was the most distinctive aspect of Y Combinator's approach to funding startups?\",\n",
              " 'What was the purpose of the Summer Founders Program mentioned in the text?',\n",
              " 'How did the Summer Founders Program help solve a problem faced by startup founders?',\n",
              " 'What factors contributed to the success of the first batch of startups funded by YC?',\n",
              " 'Why did the author believe that funding startups in batches was a scalable approach?',\n",
              " 'What were some advantages of scale that YC noticed as it grew?',\n",
              " 'Why did the author consider Hacker News to be a source of stress?',\n",
              " \"How did the author's role in YC change over time?\",\n",
              " 'What advice did Robert Morris give to the author regarding YC?',\n",
              " 'Why did the author decide to step down from running YC?',\n",
              " 'What new project did the author pursue after leaving YC?',\n",
              " \"How did the author's experience of writing essays during the development of Bel impact their understanding of the code?\",\n",
              " 'What challenges did the author face while working on Bel, and how did they overcome them?',\n",
              " \"How did the author's move to England affect the writing process of Bel?\",\n",
              " \"What is the difference between Bel and McCarthy's original Lisp in terms of implementation?\",\n",
              " 'After completing Bel, what other activities did the author engage in, and how did they decide what to work on?',\n",
              " \"How did the author's experience with Y Combinator influence their perspective on custom and tradition?\",\n",
              " 'What was the significance of the YC logo and its color choice?',\n",
              " 'Why did the author choose to self-fund Y Combinator after having enough money from the acquisition of Heroku?',\n",
              " \"How did the author's departure from Y Combinator impact their working relationship with Jessica?\",\n",
              " 'Can you explain the concept of \"discoveredness\" in relation to the development of programming languages?']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be consistent we will fix evaluation query"
      ],
      "metadata": {
        "id": "tCNyxGNYgaxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_query = 'How did the author describe their early attempts at writing short stories?'"
      ],
      "metadata": {
        "id": "AABMc2Uxgew_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix GPT-3.5-TURBO LLM for generating response\n",
        "gpt35 = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
        "service_context_gpt35 = ServiceContext.from_defaults(llm=gpt35)\n",
        "\n",
        "# Fix GPT-4 LLM for evaluation\n",
        "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n",
        "service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)"
      ],
      "metadata": {
        "id": "2XFysSdSX7pS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create vector index\n",
        "vector_index = VectorStoreIndex.from_documents(\n",
        "    documents, service_context=service_context_gpt35\n",
        ")\n",
        "\n",
        "# Query engine to generate response\n",
        "query_engine = vector_index.as_query_engine()"
      ],
      "metadata": {
        "id": "DWLP0Rk8Yj5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_index.as_retriever(similarity_top_k=3)\n",
        "nodes = retriever.retrieve(eval_query)"
      ],
      "metadata": {
        "id": "fzOT-SiFsABn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "display(HTML(f'<p style=\"font-size:20px\">{nodes[1].get_text()}</p>'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "lB9nry-UsMeR",
        "outputId": "6de5b771-9766-4805-ceec-9861dde6ecef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"font-size:20px\">What I Worked On\n",
              "\n",
              "February 2021\n",
              "\n",
              "Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n",
              "\n",
              "The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\n",
              "\n",
              "The language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\n",
              "\n",
              "I was puzzled by the 1401. I couldn't figure out what to do with it. And in retrospect there's not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn't have any data stored on punched cards. The only other option was to do things that didn't rely on any input, like calculate approximations of pi, but I didn't know enough math to do anything interesting of that type. So I'm not surprised I can't remember any programs I wrote, because they can't have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn't. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager's expression made clear.\n",
              "\n",
              "With microcomputers, everything changed. Now you could have a computer sitting right in front of you, on a desk, that could respond to your keystrokes as it was running instead of just churning through a stack of punch cards and then stopping. [1]\n",
              "\n",
              "The first of my friends to get a microcomputer built it himself. It was sold as a kit by Heathkit. I remember vividly how impressed and envious I felt watching him sitting in front of it, typing programs right into the computer.\n",
              "\n",
              "Computers were expensive in those days and it took me years of nagging before I convinced my father to buy one, a TRS-80, in about 1980. The gold standard then was the Apple II, but a TRS-80 was good enough. This was when I really started programming. I wrote simple games, a program to predict how high my model rockets would fly, and a word processor that my father used to write at least one book. There was only room in memory for about 2 pages of text, so he'd write 2 pages at a time and then print them out, but it was a lot better than a typewriter.\n",
              "\n",
              "Though I liked programming, I didn't plan to study it in college. In college I was going to study philosophy, which sounded much more powerful. It seemed, to my naive high school self, to be the study of the ultimate truths, compared to which the things studied in other fields would be mere domain knowledge. What I discovered when I got to college was that the other fields took up so much of the space of ideas that there wasn't much left for these supposed ultimate truths. All that seemed left for philosophy were edge cases that people in other fields felt could safely be ignored.\n",
              "\n",
              "I couldn't have put this into words when I was 18. All I knew at the time was that I kept taking philosophy courses and they kept being boring. So I decided to switch to AI.\n",
              "\n",
              "AI was in the air in the mid 1980s, but there were two things especially that made me want to work on it: a novel by Heinlein called The Moon is a Harsh Mistress, which featured an intelligent computer called Mike, and a PBS documentary that showed Terry Winograd using SHRDLU. I haven't tried rereading The Moon is a Harsh Mistress, so I don't know how well it has aged, but when I read it I was drawn entirely into its world. It seemed only a matter of time before we'd have Mike, and when I saw Winograd using SHRDLU, it seemed like that time would be a few years at most. All you had to do was teach SHRDLU more words.</p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Context Relevency Evaluation\n",
        "\n",
        "Measures if the response + source nodes match the query."
      ],
      "metadata": {
        "id": "8gs6eBCIX2yM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create RelevancyEvaluator using GPT-4 LLM\n",
        "relevancy_evaluator = RelevancyEvaluator(service_context=service_context_gpt4)"
      ],
      "metadata": {
        "id": "jfTwgJ5SXoeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate response\n",
        "response_vector = query_engine.query(eval_query)\n",
        "\n",
        "# Evaluation\n",
        "eval_result = relevancy_evaluator.evaluate_response(\n",
        "    query=eval_questions[1], response=response_vector\n",
        ")"
      ],
      "metadata": {
        "id": "6Bw9KWY-YflD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87f80f80-dce6-41cb-8133-3c45d1151521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=668 request_id=5474d1f3d0b2512438324a2b0ed23053 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=668 request_id=5474d1f3d0b2512438324a2b0ed23053 response_code=200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result.query"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3JMGtE-JaQWd",
        "outputId": "b15fe3b0-dc15-46f0-80d8-92219c5159c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'How did the author describe their early attempts at writing short stories?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result.response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "3irb7WZ-cW84",
        "outputId": "feb54fcd-7674-4ca7-be16-229117080737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The author described their early attempts at writing short stories as awful. They mentioned that their stories had hardly any plot and mainly focused on characters with strong feelings, which they believed made them deep.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result.passing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBFxnKgXcZlT",
        "outputId": "032b8862-40c7-4852-c3c7-136bd156ebe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relevancy evaluation with multiple source nodes."
      ],
      "metadata": {
        "id": "qaDtmBEjhGQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Query Engine with similarity_top_k=3\n",
        "query_engine = vector_index.as_query_engine(similarity_top_k=3)\n",
        "\n",
        "# Create response\n",
        "response_vector = query_engine.query(eval_query)\n",
        "\n",
        "# Evaluate with each source node\n",
        "eval_source_result_full = [\n",
        "    relevancy_evaluator.evaluate(\n",
        "        query=eval_query,\n",
        "        response=response_vector.response,\n",
        "        contexts=[source_node.get_content()],\n",
        "    )\n",
        "    for source_node in response_vector.source_nodes\n",
        "]\n",
        "\n",
        "# Evaluation result\n",
        "eval_source_result = [\n",
        "    \"Pass\" if result.passing else \"Fail\" for result in eval_source_result_full\n",
        "]"
      ],
      "metadata": {
        "id": "QOhxXsMIgVH8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "765d1750-bf2b-40ba-aab6-d0b5f4f23e1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=491 request_id=7be26f180d7395d81037252dec236c4a response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=491 request_id=7be26f180d7395d81037252dec236c4a response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=941 request_id=151ce961ee2cebc33045717eeeaa3551 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=941 request_id=151ce961ee2cebc33045717eeeaa3551 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=781 request_id=db9a818ee0474c4b1f868ed65372fbd5 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=781 request_id=db9a818ee0474c4b1f868ed65372fbd5 response_code=200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_source_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-euW1W3VhgPx",
        "outputId": "20dca14e-c510-4a80-f570-81e162c77a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Fail', 'Pass', 'Fail']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Faithfullness Evaluator\n",
        "\n",
        " Measures if the response from a query engine matches any source nodes. This is useful for measuring if the response was hallucinated."
      ],
      "metadata": {
        "id": "Rrd_7kufgozj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "faithfulness_evaluator = FaithfulnessEvaluator(service_context=service_context_gpt4)"
      ],
      "metadata": {
        "id": "Pb3d08hrclbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result = faithfulness_evaluator.evaluate_response(response=response_vector)"
      ],
      "metadata": {
        "id": "hWOKhDVTdm5q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72034782-9612-4221-a632-e24810b7b021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1181 request_id=013b8f59b2bd0386e84c11de4d47a325 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=1181 request_id=013b8f59b2bd0386e84c11de4d47a325 response_code=200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yH977tlPk6HQ",
        "outputId": "4201e8fb-8f50-482b-d7a8-88848144726d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EvaluationResult(query=None, contexts=[\"[10]\\n\\nWow, I thought, there's an audience. If I write something and put it on the web, anyone can read it. That may seem obvious now, but it was surprising then. In the print era there was a narrow channel to readers, guarded by fierce monsters known as editors. The only way to get an audience for anything you wrote was to get it published as a book, or in a newspaper or magazine. Now anyone could publish anything.\\n\\nThis had been possible in principle since 1993, but not many people had realized it yet. I had been intimately involved with building the infrastructure of the web for most of that time, and a writer as well, and it had taken me 8 years to realize it. Even then it took me several years to understand the implications. It meant there would be a whole new generation of essays. [11]\\n\\nIn the print era, the channel for publishing essays had been vanishingly small. Except for a few officially anointed thinkers who went to the right parties in New York, the only people allowed to publish essays were specialists writing about their specialties. There were so many essays that had never been written, because there had been no way to publish them. Now they could be, and I was going to write them. [12]\\n\\nI've worked on several different things, but to the extent there was a turning point where I figured out what to work on, it was when I started publishing essays online. From then on I knew that whatever else I did, I'd always write essays too.\\n\\nI knew that online essays would be a marginal medium at first. Socially they'd seem more like rants posted by nutjobs on their GeoCities sites than the genteel and beautifully typeset compositions published in The New Yorker. But by this point I knew enough to find that encouraging instead of discouraging.\\n\\nOne of the most conspicuous patterns I've noticed in my life is how well it has worked, for me at least, to work on things that weren't prestigious. Still life has always been the least prestigious form of painting. Viaweb and Y Combinator both seemed lame when we started them. I still get the glassy eye from strangers when they ask what I'm writing, and I explain that it's an essay I'm going to publish on my web site. Even Lisp, though prestigious intellectually in something like the way Latin is, also seems about as hip.\\n\\nIt's not that unprestigious types of work are good per se. But when you find yourself drawn to some kind of work despite its current lack of prestige, it's a sign both that there's something real to be discovered there, and that you have the right kind of motives. Impure motives are a big danger for the ambitious. If anything is going to lead you astray, it will be the desire to impress people. So while working on things that aren't prestigious doesn't guarantee you're on the right track, it at least guarantees you're not on the most common type of wrong one.\\n\\nOver the next several years I wrote lots of essays about all kinds of different topics. O'Reilly reprinted a collection of them as a book, called Hackers & Painters after one of the essays in it. I also worked on spam filters, and did some more painting. I used to have dinners for a group of friends every thursday night, which taught me how to cook for groups. And I bought another building in Cambridge, a former candy factory (and later, twas said, porn studio), to use as an office.\\n\\nOne night in October 2003 there was a big party at my house. It was a clever idea of my friend Maria Daniels, who was one of the thursday diners. Three separate hosts would all invite their friends to one party. So for every guest, two thirds of the other guests would be people they didn't know but would probably like. One of the guests was someone I didn't know but would turn out to like a lot: a woman called Jessica Livingston. A couple days later I asked her out.\\n\\nJessica was in charge of marketing at a Boston investment bank. This bank thought it understood startups, but over the next year, as she met friends of mine from the startup world, she was surprised how different reality was. And how colorful their stories were. So she decided to compile a book of interviews with startup founders.\\n\\nWhen the bank had financial problems and she had to fire half her staff, she started looking for a new job. In early 2005 she interviewed for a marketing job at a Boston VC firm. It took them weeks to make up their minds, and during this time I started telling her about all the things that needed to be fixed about venture capital.\", 'What I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\\n\\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\\n\\nI was puzzled by the 1401. I couldn\\'t figure out what to do with it. And in retrospect there\\'s not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn\\'t have any data stored on punched cards. The only other option was to do things that didn\\'t rely on any input, like calculate approximations of pi, but I didn\\'t know enough math to do anything interesting of that type. So I\\'m not surprised I can\\'t remember any programs I wrote, because they can\\'t have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn\\'t. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager\\'s expression made clear.\\n\\nWith microcomputers, everything changed. Now you could have a computer sitting right in front of you, on a desk, that could respond to your keystrokes as it was running instead of just churning through a stack of punch cards and then stopping. [1]\\n\\nThe first of my friends to get a microcomputer built it himself. It was sold as a kit by Heathkit. I remember vividly how impressed and envious I felt watching him sitting in front of it, typing programs right into the computer.\\n\\nComputers were expensive in those days and it took me years of nagging before I convinced my father to buy one, a TRS-80, in about 1980. The gold standard then was the Apple II, but a TRS-80 was good enough. This was when I really started programming. I wrote simple games, a program to predict how high my model rockets would fly, and a word processor that my father used to write at least one book. There was only room in memory for about 2 pages of text, so he\\'d write 2 pages at a time and then print them out, but it was a lot better than a typewriter.\\n\\nThough I liked programming, I didn\\'t plan to study it in college. In college I was going to study philosophy, which sounded much more powerful. It seemed, to my naive high school self, to be the study of the ultimate truths, compared to which the things studied in other fields would be mere domain knowledge. What I discovered when I got to college was that the other fields took up so much of the space of ideas that there wasn\\'t much left for these supposed ultimate truths. All that seemed left for philosophy were edge cases that people in other fields felt could safely be ignored.\\n\\nI couldn\\'t have put this into words when I was 18. All I knew at the time was that I kept taking philosophy courses and they kept being boring. So I decided to switch to AI.\\n\\nAI was in the air in the mid 1980s, but there were two things especially that made me want to work on it: a novel by Heinlein called The Moon is a Harsh Mistress, which featured an intelligent computer called Mike, and a PBS documentary that showed Terry Winograd using SHRDLU. I haven\\'t tried rereading The Moon is a Harsh Mistress, so I don\\'t know how well it has aged, but when I read it I was drawn entirely into its world. It seemed only a matter of time before we\\'d have Mike, and when I saw Winograd using SHRDLU, it seemed like that time would be a few years at most. All you had to do was teach SHRDLU more words.', 'Not so much because it was badly written as because the problem is so convoluted. When you\\'re working on an interpreter written in itself, it\\'s hard to keep track of what\\'s happening at what level, and errors can be practically encrypted by the time you get them.\\n\\nSo I said no more essays till Bel was done. But I told few people about Bel while I was working on it. So for years it must have seemed that I was doing nothing, when in fact I was working harder than I\\'d ever worked on anything. Occasionally after wrestling for hours with some gruesome bug I\\'d check Twitter or HN and see someone asking \"Does Paul Graham still code?\"\\n\\nWorking on Bel was hard but satisfying. I worked on it so intensively that at any given time I had a decent chunk of the code in my head and could write more there. I remember taking the boys to the coast on a sunny day in 2015 and figuring out how to deal with some problem involving continuations while I watched them play in the tide pools. It felt like I was doing life right. I remember that because I was slightly dismayed at how novel it felt. The good news is that I had more moments like this over the next few years.\\n\\nIn the summer of 2016 we moved to England. We wanted our kids to see what it was like living in another country, and since I was a British citizen by birth, that seemed the obvious choice. We only meant to stay for a year, but we liked it so much that we still live there. So most of Bel was written in England.\\n\\nIn the fall of 2019, Bel was finally finished. Like McCarthy\\'s original Lisp, it\\'s a spec rather than an implementation, although like McCarthy\\'s Lisp it\\'s a spec expressed as code.\\n\\nNow that I could write essays again, I wrote a bunch about topics I\\'d had stacked up. I kept writing essays through 2020, but I also started to think about other things I could work on. How should I choose what to do? Well, how had I chosen what to work on in the past? I wrote an essay for myself to answer that question, and I was surprised how long and messy the answer turned out to be. If this surprised me, who\\'d lived it, then I thought perhaps it would be interesting to other people, and encouraging to those with similarly messy lives. So I wrote a more detailed version for others to read, and this is the last sentence of it.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNotes\\n\\n[1] My experience skipped a step in the evolution of computers: time-sharing machines with interactive OSes. I went straight from batch processing to microcomputers, which made microcomputers seem all the more exciting.\\n\\n[2] Italian words for abstract concepts can nearly always be predicted from their English cognates (except for occasional traps like polluzione). It\\'s the everyday words that differ. So if you string together a lot of abstract concepts with a few simple verbs, you can make a little Italian go a long way.\\n\\n[3] I lived at Piazza San Felice 4, so my walk to the Accademia went straight down the spine of old Florence: past the Pitti, across the bridge, past Orsanmichele, between the Duomo and the Baptistery, and then up Via Ricasoli to Piazza San Marco. I saw Florence at street level in every possible condition, from empty dark winter evenings to sweltering summer days when the streets were packed with tourists.\\n\\n[4] You can of course paint people like still lives if you want to, and they\\'re willing. That sort of portrait is arguably the apex of still life painting, though the long sitting does tend to produce pained expressions in the sitters.\\n\\n[5] Interleaf was one of many companies that had smart people and built impressive technology, and yet got crushed by Moore\\'s Law. In the 1990s the exponential growth in the power of commodity (i.e. Intel) processors rolled up high-end, special-purpose hardware and software companies like a bulldozer.\\n\\n[6] The signature style seekers at RISD weren\\'t specifically mercenary. In the art world, money and coolness are tightly coupled. Anything expensive comes to be seen as cool, and anything seen as cool will soon become equally expensive.\\n\\n[7] Technically the apartment wasn\\'t rent-controlled but rent-stabilized, but this is a refinement only New Yorkers would know or care about. The point is that it was really cheap, less than half market price.\\n\\n[8] Most software you can launch as soon as it\\'s done. But when the software is an online store builder and you\\'re hosting the stores, if you don\\'t have any users yet, that fact will be painfully obvious.'], response='The author described their early attempts at writing short stories as awful. They mentioned that their stories had hardly any plot and mainly focused on characters with strong feelings, which they believed made the stories deep.', passing=True, feedback='YES', score=1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result.passing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsiWpNXHdsoz",
        "outputId": "47e11e7b-b9ba-4815-e62d-234a0e6a0d3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Correctness Evaluator\n",
        "\n",
        "Evaluates the relevance and correctness of a generated answer against a reference answer."
      ],
      "metadata": {
        "id": "SqsEWzF1i1Rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correctness_evaluator = CorrectnessEvaluator(service_context=service_context_gpt4)"
      ],
      "metadata": {
        "id": "ZdX4-K-NfNIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = (\n",
        "    \"Can you explain the theory of relativity proposed by Albert Einstein in detail?\"\n",
        ")\n",
        "\n",
        "reference = \"\"\"\n",
        "Certainly! Albert Einstein's theory of relativity consists of two main components: special relativity and general relativity. Special relativity, published in 1905, introduced the concept that the laws of physics are the same for all non-accelerating observers and that the speed of light in a vacuum is a constant, regardless of the motion of the source or observer. It also gave rise to the famous equation E=mc², which relates energy (E) and mass (m).\n",
        "\n",
        "General relativity, published in 1915, extended these ideas to include the effects of gravity. According to general relativity, gravity is not a force between masses, as described by Newton's theory of gravity, but rather the result of the warping of space and time by mass and energy. Massive objects, such as planets and stars, cause a curvature in spacetime, and smaller objects follow curved paths in response to this curvature. This concept is often illustrated using the analogy of a heavy ball placed on a rubber sheet, causing it to create a depression that other objects (representing smaller masses) naturally move towards.\n",
        "\n",
        "In essence, general relativity provided a new understanding of gravity, explaining phenomena like the bending of light by gravity (gravitational lensing) and the precession of the orbit of Mercury. It has been confirmed through numerous experiments and observations and has become a fundamental theory in modern physics.\n",
        "\"\"\"\n",
        "\n",
        "response = \"\"\"\n",
        "Certainly! Albert Einstein's theory of relativity consists of two main components: special relativity and general relativity. Special relativity, published in 1905, introduced the concept that the laws of physics are the same for all non-accelerating observers and that the speed of light in a vacuum is a constant, regardless of the motion of the source or observer. It also gave rise to the famous equation E=mc², which relates energy (E) and mass (m).\n",
        "\n",
        "However, general relativity, published in 1915, extended these ideas to include the effects of magnetism. According to general relativity, gravity is not a force between masses but rather the result of the warping of space and time by magnetic fields generated by massive objects. Massive objects, such as planets and stars, create magnetic fields that cause a curvature in spacetime, and smaller objects follow curved paths in response to this magnetic curvature. This concept is often illustrated using the analogy of a heavy ball placed on a rubber sheet with magnets underneath, causing it to create a depression that other objects (representing smaller masses) naturally move towards due to magnetic attraction.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "pbsExY8qi9ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correctness_result = correctness_evaluator.evaluate(\n",
        "    query=query,\n",
        "    response=response,\n",
        "    reference=reference,\n",
        ")"
      ],
      "metadata": {
        "id": "6fOKYRacjIkm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dce527e-efaa-4129-a9d9-f9fb4eca89b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=5841 request_id=6125174196a7946bea6aa8249bc8ff68 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=5841 request_id=6125174196a7946bea6aa8249bc8ff68 response_code=200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correctness_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhEnvJtIjL91",
        "outputId": "0f8ed066-8437-47ad-8a63-6416d54a20ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EvaluationResult(query='Can you explain the theory of relativity proposed by Albert Einstein in detail?', contexts=None, response=\"\\nCertainly! Albert Einstein's theory of relativity consists of two main components: special relativity and general relativity. Special relativity, published in 1905, introduced the concept that the laws of physics are the same for all non-accelerating observers and that the speed of light in a vacuum is a constant, regardless of the motion of the source or observer. It also gave rise to the famous equation E=mc², which relates energy (E) and mass (m).\\n\\nHowever, general relativity, published in 1915, extended these ideas to include the effects of magnetism. According to general relativity, gravity is not a force between masses but rather the result of the warping of space and time by magnetic fields generated by massive objects. Massive objects, such as planets and stars, create magnetic fields that cause a curvature in spacetime, and smaller objects follow curved paths in response to this magnetic curvature. This concept is often illustrated using the analogy of a heavy ball placed on a rubber sheet with magnets underneath, causing it to create a depression that other objects (representing smaller masses) naturally move towards due to magnetic attraction.\\n\", passing=False, feedback='The generated answer is relevant to the user query and starts off correctly by explaining special relativity. However, it contains significant mistakes when explaining general relativity. General relativity is about the warping of space and time by mass and energy, not magnetic fields. The analogy used is also incorrect as it introduces magnets, which are not part of the original concept.', score=2.5)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correctness_result.score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbWAp_krjSCI",
        "outputId": "d9139ab1-2c04-4ef8-dadc-66c5d5f0acba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.5"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correctness_result.passing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBk5I1i9jWM_",
        "outputId": "56bf807d-26a7-4718-b04e-f7e740d72b00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correctness_result.feedback"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "66aEQczhjXpO",
        "outputId": "01cbce80-9023-458b-a316-170f33489c14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The generated answer is relevant to the user query and starts off correctly by explaining special relativity. However, it contains significant mistakes when explaining general relativity. General relativity is about the warping of space and time by mass and energy, not magnetic fields. The analogy used is also incorrect as it introduces magnets, which are not part of the original concept.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Guideline Evaluator\n",
        "\n",
        "Evaluates a question answer system given user specified guidelines."
      ],
      "metadata": {
        "id": "P7EcLsoVjmZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GUIDELINES = [\n",
        "    \"The response should fully answer the query.\",\n",
        "    \"The response should avoid being vague or ambiguous.\",\n",
        "    \"The response should be specific and use statistics or numbers when possible.\",\n",
        "]"
      ],
      "metadata": {
        "id": "sotyCAmCjZbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluators = [\n",
        "    GuidelineEvaluator(service_context=service_context_gpt4, guidelines=guideline)\n",
        "    for guideline in GUIDELINES\n",
        "]"
      ],
      "metadata": {
        "id": "uzuURTW-jrFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data = {\n",
        "    \"query\": \"Tell me about global warming.\",\n",
        "    \"contexts\": [\n",
        "        \"Global warming refers to the long-term increase in Earth's average surface temperature due to human activities such as the burning of fossil fuels and deforestation.\",\n",
        "        \"It is a major environmental issue with consequences such as rising sea levels, extreme weather events, and disruptions to ecosystems.\",\n",
        "        \"Efforts to combat global warming include reducing carbon emissions, transitioning to renewable energy sources, and promoting sustainable practices.\",\n",
        "    ],\n",
        "    \"response\": \"Global warming is a critical environmental issue caused by human activities that lead to a rise in Earth's temperature. It has various adverse effects on the planet.\",\n",
        "}"
      ],
      "metadata": {
        "id": "Vktd6BV3jxSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for guideline, evaluator in zip(GUIDELINES, evaluators):\n",
        "    eval_result = evaluator.evaluate(\n",
        "        query=sample_data[\"query\"],\n",
        "        contexts=sample_data[\"contexts\"],\n",
        "        response=sample_data[\"response\"],\n",
        "    )\n",
        "    print(\"=====\")\n",
        "    print(f\"Guideline: {guideline}\")\n",
        "    print(f\"Pass: {eval_result.passing}\")\n",
        "    print(f\"Feedback: {eval_result.feedback}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKk0jpFwjzp9",
        "outputId": "65ba6d52-3262-4349-8f73-3283afff5c21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=6244 request_id=7989fda2339313abe75537653e3acc1b response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=6244 request_id=7989fda2339313abe75537653e3acc1b response_code=200\n",
            "=====\n",
            "Guideline: The response should fully answer the query.\n",
            "Pass: False\n",
            "Feedback: The response does not fully answer the query. While it does provide a brief overview of global warming, it does not delve into the specifics such as the causes, effects, and potential solutions to global warming. The response should be more detailed and comprehensive to fully answer the query.\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=5081 request_id=6cac8bf18d0fa819278f24f163ec894c response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=5081 request_id=6cac8bf18d0fa819278f24f163ec894c response_code=200\n",
            "=====\n",
            "Guideline: The response should avoid being vague or ambiguous.\n",
            "Pass: False\n",
            "Feedback: The response is too vague and does not provide specific details about global warming. It should include more information about the causes, effects, and potential solutions to global warming.\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=6299 request_id=62cc8eee141e7fe5f7a50a0f0ba486b5 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/chat/completions processing_ms=6299 request_id=62cc8eee141e7fe5f7a50a0f0ba486b5 response_code=200\n",
            "=====\n",
            "Guideline: The response should be specific and use statistics or numbers when possible.\n",
            "Pass: False\n",
            "Feedback: The response is too general and lacks specific details. It does not include any statistics or numbers to support the information provided about global warming. The response could be improved by including data such as the rate of temperature increase, the amount of greenhouse gases emitted annually, or the projected impacts of global warming.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hit Rate:\n",
        "MRR:\n",
        "\n",
        "Document -> D\n",
        "\n",
        "D -> N1, N2, N3, N4, N5 -> Index/ Retriever\n",
        "\n",
        "(Q1, N1)\n",
        "(Q2, N1)\n",
        "(Q3, N2)\n",
        "(Q4, N2)\n",
        "(Q5, N3)\n",
        "(Q6, N3)\n",
        "(Q7, N4)\n",
        "(Q8, N4)\n",
        "(Q9, N5)\n",
        "(Q10, N5)\n",
        "\n",
        "Q1 -> Index/ Retriever -> N2, N1, N3 -> 1 -> 1/2\n",
        "\n",
        "Q2 -> Index/ Retriever -> N5, N4, N3 -> 0 -> 0\n",
        "\n",
        "Q3 -> Index/ Retriever -> N1, N2, N3 -> 1 -> 1/2\n",
        "\n",
        "Q4 -> Index/ Retriever -> N2, N3, N5 -> 1 -> 1/1\n",
        "\n",
        "Q5 -> Index/ Retriever -> N3, N1, N4 -> 1 -> 1/1\n",
        "\n",
        "Q6 -> Index/ Retriever -> N1, N2, N3 -> 1 -> 1/3\n",
        "\n",
        "Q7 -> Index/ Retriever -> N4, N1, N2 -> 1 -> 1/1\n",
        "\n",
        "Q8 -> Index/ Retriever -> N1, N3, N4 -> 1 -> 1/3\n",
        "\n",
        "Q9 -> Index/ Retriever -> N2, N3, N4 -> 0 -> 0\n",
        "\n",
        "Q10 -> Index/ Retriever -> N2, N5, N3 -> 1 -> 1/2\n",
        "\n",
        "Hit Rate: 8/10 -> 80%\n",
        "\n",
        "MRR: (0.5 + 0 + 0.5 + 1 + 1 + 0.33 + 1 + 0.33 + 0 + 0.5)/10 -> 0.55"
      ],
      "metadata": {
        "id": "qXKPpQ0tP76P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval Evaluation\n",
        "\n",
        "Evaluates the quality of any Retriever module defined in LlamaIndex.\n",
        "\n",
        "To assess the quality of a Retriever module in LlamaIndex, we use metrics like hit-rate and MRR. These compare retrieved results to ground-truth context for any question. For simpler evaluation dataset creation, we utilize synthetic data generation."
      ],
      "metadata": {
        "id": "aX7xg4hAohDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reader = SimpleDirectoryReader(\"./data/paul_graham/\")\n",
        "documents = reader.load_data()\n",
        "\n",
        "node_parser = SimpleNodeParser.from_defaults(chunk_size=512)\n",
        "nodes = node_parser.get_nodes_from_documents(documents)"
      ],
      "metadata": {
        "id": "wFtWgxqj1x7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_index = VectorStoreIndex(nodes, service_context=service_context_gpt4)"
      ],
      "metadata": {
        "id": "L8WLcpA-12LB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the retriever\n",
        "retriever = vector_index.as_retriever(similarity_top_k=2)"
      ],
      "metadata": {
        "id": "XZllZ2u5oj0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_nodes = retriever.retrieve(eval_query)"
      ],
      "metadata": {
        "id": "A5R3H3erqx_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.response.notebook_utils import display_source_node\n",
        "\n",
        "for node in retrieved_nodes:\n",
        "    display_source_node(node, source_length=2000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        },
        "id": "VMHGPbUiqned",
        "outputId": "8c65a596-f548-489f-ef4f-1a19fec31e90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Node ID:** 221d51e9-22fd-4745-af04-89f24da138e9<br>**Similarity:** 0.827003857875047<br>**Text:** What I Worked On\n\nFebruary 2021\n\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\n\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\n\nI was puzzled by the 1401. I couldn't figure out what to do with it. And in retrospect there's not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn't have any data stored on punched cards. The only other option was to do things that didn't rely on any input, like calculate approximations of pi, but I didn't know enough math to do anything interesting of that type. So I'm not surprised I can't remember any programs I wrote, because they can't have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn't. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager's expression made clear.\n\nWith microcomputers, everything changed. Now you could h...<br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Node ID:** 252c8c8a-dafd-4d71-b9a5-baab20a245a4<br>**Similarity:** 0.8202532618916515<br>**Text:** Now they could be, and I was going to write them. [12]\n\nI've worked on several different things, but to the extent there was a turning point where I figured out what to work on, it was when I started publishing essays online. From then on I knew that whatever else I did, I'd always write essays too.\n\nI knew that online essays would be a marginal medium at first. Socially they'd seem more like rants posted by nutjobs on their GeoCities sites than the genteel and beautifully typeset compositions published in The New Yorker. But by this point I knew enough to find that encouraging instead of discouraging.\n\nOne of the most conspicuous patterns I've noticed in my life is how well it has worked, for me at least, to work on things that weren't prestigious. Still life has always been the least prestigious form of painting. Viaweb and Y Combinator both seemed lame when we started them. I still get the glassy eye from strangers when they ask what I'm writing, and I explain that it's an essay I'm going to publish on my web site. Even Lisp, though prestigious intellectually in something like the way Latin is, also seems about as hip.\n\nIt's not that unprestigious types of work are good per se. But when you find yourself drawn to some kind of work despite its current lack of prestige, it's a sign both that there's something real to be discovered there, and that you have the right kind of motives. Impure motives are a big danger for the ambitious. If anything is going to lead you astray, it will be the desire to impress people. So while working on things that aren't prestigious doesn't guarantee you're on the right track, it at least guarantees you're not on the most common type of wrong one.\n\nOver the next several years I wrote lots of essays about all kinds of different topics. O'Reilly reprinted a collection of them as a book, called Hackers & Painters after one of the essays in it. I also worked on spam filters, and did some more painting. I used to have dinners for a group...<br>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_dataset = generate_question_context_pairs(nodes, llm=gpt4, num_questions_per_chunk=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2EzD6m_rDU4",
        "outputId": "f3c20416-7a3b-4c09-ad65-3d3f6332d783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 36/36 [04:17<00:00,  7.16s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "queries = qa_dataset.queries.values()\n",
        "print(list(queries)[50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9sidRY0xNz-",
        "outputId": "76ca83f0-1544-4cdd-c6e8-e840cc4f0565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Discuss the initial investment model of Y Combinator (YC) for startups and explain how it was considered fair for both the investors and the founders.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(list(queries))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecH5iAUrTaAW",
        "outputId": "ac306671-353b-4e4b-a27d-10d228f72d5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "72"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
        "    [\"mrr\", \"hit_rate\"], retriever=retriever\n",
        ")"
      ],
      "metadata": {
        "id": "mm5GCNZoriDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try it out on a sample query\n",
        "sample_id, sample_query = list(qa_dataset.queries.items())[0]\n",
        "sample_expected = qa_dataset.relevant_docs[sample_id]\n",
        "\n",
        "eval_result = retriever_evaluator.evaluate(sample_query, sample_expected)\n",
        "print(eval_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yb5p06r0xYOR",
        "outputId": "63a434da-277b-4b9b-b0a4-728bccb3e975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=23 request_id=f3e496212e2eb4caa147464c6ab0c169 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=23 request_id=f3e496212e2eb4caa147464c6ab0c169 response_code=200\n",
            "Query: In the context, the author mentions his early experiences with programming on an IBM 1401. Describe the process he used to write and run a program on this machine, and explain why he found it challenging to create meaningful programs on this system.\n",
            "Metrics: {'mrr': 1.0, 'hit_rate': 1.0}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# try it out on an entire dataset\n",
        "eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)"
      ],
      "metadata": {
        "id": "MF4-RWsDrnMJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d62d3f2-8a02-40ff-f624-f2e182da7c94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=20 request_id=8f8769d000d3d977aeb8922b78f6f0ec response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=20 request_id=8f8769d000d3d977aeb8922b78f6f0ec response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=29 request_id=ab82fcb4ebba619cd6057c9abbd5d70e response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=29 request_id=ab82fcb4ebba619cd6057c9abbd5d70e response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=23 request_id=961ec5e8d6327868053c616d7c2969c2 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=23 request_id=961ec5e8d6327868053c616d7c2969c2 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=39 request_id=041d06972fdd38cbf9a6210f71f3f82d response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=39 request_id=041d06972fdd38cbf9a6210f71f3f82d response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=23 request_id=c3ca1c7e42d4261ec94d3ded6ff391ea response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=23 request_id=c3ca1c7e42d4261ec94d3ded6ff391ea response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=43 request_id=e88676024996728de0b6d7d8fa3f9ec1 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=43 request_id=e88676024996728de0b6d7d8fa3f9ec1 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=19 request_id=7f0049d669cd6b4e7a77b35f1bd176ed response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=19 request_id=7f0049d669cd6b4e7a77b35f1bd176ed response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=35 request_id=af9d2d24842fcb5bfda5f87f9a637374 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=35 request_id=af9d2d24842fcb5bfda5f87f9a637374 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=60 request_id=ec0d35829b6824a306e36aa405c0febd response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=60 request_id=ec0d35829b6824a306e36aa405c0febd response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=83 request_id=5d023d3cf1ea4edba913c9743bb3abae response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=83 request_id=5d023d3cf1ea4edba913c9743bb3abae response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=30 request_id=0d1e1de3635325e12c940ec55b90021f response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=30 request_id=0d1e1de3635325e12c940ec55b90021f response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=17 request_id=5dc07594f290bab006d08ca9d6814a17 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=17 request_id=5dc07594f290bab006d08ca9d6814a17 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=19 request_id=afe85f78270a15dec11b561c536c3eb5 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=19 request_id=afe85f78270a15dec11b561c536c3eb5 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=37 request_id=8f1b70966fb7ac77f410e1bff8299a4d response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=37 request_id=8f1b70966fb7ac77f410e1bff8299a4d response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=23 request_id=f5fd3b3101a887f49da87a9d07a7ccf4 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=23 request_id=f5fd3b3101a887f49da87a9d07a7ccf4 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=18 request_id=384a690d3fcd5cd7f4db19f7761c1147 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=18 request_id=384a690d3fcd5cd7f4db19f7761c1147 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=26 request_id=e05d8e4fe40fdf22b01740befd51d2cf response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=26 request_id=e05d8e4fe40fdf22b01740befd51d2cf response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=31 request_id=f4adec3e7b4ae7e6e9df200c954a5810 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=31 request_id=f4adec3e7b4ae7e6e9df200c954a5810 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=30 request_id=efbc231629a3123fc933cf8031ad6048 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=30 request_id=efbc231629a3123fc933cf8031ad6048 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=19 request_id=5b9f4b92d9c94a484667c5eb5e08ce06 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=19 request_id=5b9f4b92d9c94a484667c5eb5e08ce06 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=65 request_id=e2177c3e91282b9e0029c5a2c4978a96 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=65 request_id=e2177c3e91282b9e0029c5a2c4978a96 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=19 request_id=1aa4dc8a569c04a7f520647bba71cb38 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=19 request_id=1aa4dc8a569c04a7f520647bba71cb38 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=76 request_id=bb7fc6467f73004f547d489fa26178c3 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=76 request_id=bb7fc6467f73004f547d489fa26178c3 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=19 request_id=9c8fed4fc21a37839f0c244756362bdc response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=19 request_id=9c8fed4fc21a37839f0c244756362bdc response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=28 request_id=b30c8bdc1b663141480f119b9a189587 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=28 request_id=b30c8bdc1b663141480f119b9a189587 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=31 request_id=dbb8cedbd609ec5ddfb17170975f7f31 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=31 request_id=dbb8cedbd609ec5ddfb17170975f7f31 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=25 request_id=8edd9f713d37945414d32a1d44c7ecd2 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=25 request_id=8edd9f713d37945414d32a1d44c7ecd2 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=22 request_id=1ba95c459d96bc1633681ff45a747d8c response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=22 request_id=1ba95c459d96bc1633681ff45a747d8c response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=19 request_id=3f29c05fbbabeac234a508f423befc81 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=19 request_id=3f29c05fbbabeac234a508f423befc81 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=32 request_id=adc214d3f391be09e650d83068c3e9df response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=32 request_id=adc214d3f391be09e650d83068c3e9df response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=17 request_id=8c04207fa792997e01944dc062deb61c response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=17 request_id=8c04207fa792997e01944dc062deb61c response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=44 request_id=3869044cc39b9d7756e5899b644c2632 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=44 request_id=3869044cc39b9d7756e5899b644c2632 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=25 request_id=19c4632e1b55b7110a044ffe0646fb0a response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=25 request_id=19c4632e1b55b7110a044ffe0646fb0a response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=25 request_id=ef5fbe5fbec2ccb81c3ee65d3e7adaf3 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=25 request_id=ef5fbe5fbec2ccb81c3ee65d3e7adaf3 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=17 request_id=245e532dd8f59052d54ea5fe9307f160 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=17 request_id=245e532dd8f59052d54ea5fe9307f160 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=19 request_id=680c256cab2ca64bfc6de668f11904ea response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=19 request_id=680c256cab2ca64bfc6de668f11904ea response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=24 request_id=e7144edd9aeffddc9e81bd13f9ac8abd response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=24 request_id=e7144edd9aeffddc9e81bd13f9ac8abd response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=140 request_id=4c336775bf232f5eb61d31127e04e981 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=140 request_id=4c336775bf232f5eb61d31127e04e981 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=42 request_id=9c6e7bbfb1d1f59b99740af7c86385f6 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=42 request_id=9c6e7bbfb1d1f59b99740af7c86385f6 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=57 request_id=2426dcb428ec9755be31e478d92b68a9 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=57 request_id=2426dcb428ec9755be31e478d92b68a9 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=33 request_id=7eff6745721c7f4fb5b25af85c76e312 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=33 request_id=7eff6745721c7f4fb5b25af85c76e312 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=49 request_id=8003ea65289ee4b382c9c4023131a5d3 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=49 request_id=8003ea65289ee4b382c9c4023131a5d3 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=29 request_id=2f1eeaf905f4a1a9940f4e77bee7ea7b response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=29 request_id=2f1eeaf905f4a1a9940f4e77bee7ea7b response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=55 request_id=443e736d426b92e08956543b70edb812 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=55 request_id=443e736d426b92e08956543b70edb812 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=66 request_id=74422717dfbbddb1e8f8c9b2cfa1043d response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=66 request_id=74422717dfbbddb1e8f8c9b2cfa1043d response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=21 request_id=c3c48d0ddf168a87d6553581d6998dde response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=21 request_id=c3c48d0ddf168a87d6553581d6998dde response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=64 request_id=9fe8a820af133eefaf3fa08c6aee39d4 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=64 request_id=9fe8a820af133eefaf3fa08c6aee39d4 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=36 request_id=bf4ade0b60c049e04469935c303dcd9f response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=36 request_id=bf4ade0b60c049e04469935c303dcd9f response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=29 request_id=05daba0dfb3db85cc429c27a965cb39b response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=29 request_id=05daba0dfb3db85cc429c27a965cb39b response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=20 request_id=8f527803b64a9def6c466ba5a5b4d62b response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=20 request_id=8f527803b64a9def6c466ba5a5b4d62b response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=34 request_id=11a4f9464f77add89a1e7f53ccbb6278 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=34 request_id=11a4f9464f77add89a1e7f53ccbb6278 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=17 request_id=d4bcf23b86fa5597ed167b85c2f55a7a response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=17 request_id=d4bcf23b86fa5597ed167b85c2f55a7a response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=28 request_id=94dec0216d687425a010166f8c5374e8 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=28 request_id=94dec0216d687425a010166f8c5374e8 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=19 request_id=d3cae8e6c9cd3d77a062d8a09ae30b13 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=19 request_id=d3cae8e6c9cd3d77a062d8a09ae30b13 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=25 request_id=0bb967730e993f608eb86e74b04ec911 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=25 request_id=0bb967730e993f608eb86e74b04ec911 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=22 request_id=3ece6efb55e196a6130342d2aa9999b0 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=22 request_id=3ece6efb55e196a6130342d2aa9999b0 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=237 request_id=efa5683c2b31abf4ef6edc46405bef93 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=237 request_id=efa5683c2b31abf4ef6edc46405bef93 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=24 request_id=c214addc7c6c14c85a838d1398353201 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=24 request_id=c214addc7c6c14c85a838d1398353201 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=22 request_id=855e9183d03f20e200cee2f001501c8b response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=22 request_id=855e9183d03f20e200cee2f001501c8b response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=35 request_id=20673cae3898aaf4c4069d76be69ce60 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=35 request_id=20673cae3898aaf4c4069d76be69ce60 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=27 request_id=4716f2bd84cac8658240d3451b323040 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=27 request_id=4716f2bd84cac8658240d3451b323040 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=20 request_id=3dec7d138af87e82603fc1048a56342e response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=20 request_id=3dec7d138af87e82603fc1048a56342e response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=30 request_id=da6c2c40e2333dd73a9df4375b374258 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=30 request_id=da6c2c40e2333dd73a9df4375b374258 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=25 request_id=cccd529a7730ed31709b31f4a33211c7 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=25 request_id=cccd529a7730ed31709b31f4a33211c7 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=22 request_id=fa7cb2175a01a6cbc57eb8960ce897d1 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=22 request_id=fa7cb2175a01a6cbc57eb8960ce897d1 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=23 request_id=6613af8ac9335fde1f7b1dd1c0358f4e response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=23 request_id=6613af8ac9335fde1f7b1dd1c0358f4e response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=62 request_id=789e4ad78f51a01a912ae243f5bf7050 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=62 request_id=789e4ad78f51a01a912ae243f5bf7050 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=20 request_id=a1ba8195ee66f88ac0a489d6f3f03129 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=20 request_id=a1ba8195ee66f88ac0a489d6f3f03129 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=42 request_id=dee2858cca3a7b4822ae78a7900b8471 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=42 request_id=dee2858cca3a7b4822ae78a7900b8471 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=74 request_id=e83788f0935c9d69c4a62f2d4bc5bdc1 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=74 request_id=e83788f0935c9d69c4a62f2d4bc5bdc1 response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=21 request_id=cc4b3b60d404489b622697a81b0fbf1b response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=21 request_id=cc4b3b60d404489b622697a81b0fbf1b response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=59 request_id=e43de8ca5e0dcc418bc72e24fb059c9f response_code=200\n",
            "message='OpenAI API response' path=https://api.openai.com/v1/embeddings processing_ms=59 request_id=e43de8ca5e0dcc418bc72e24fb059c9f response_code=200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def display_results(name, eval_results):\n",
        "    \"\"\"Display results from evaluate.\"\"\"\n",
        "\n",
        "    metric_dicts = []\n",
        "    for eval_result in eval_results:\n",
        "        metric_dict = eval_result.metric_vals_dict\n",
        "        metric_dicts.append(metric_dict)\n",
        "\n",
        "    full_df = pd.DataFrame(metric_dicts)\n",
        "\n",
        "    hit_rate = full_df[\"hit_rate\"].mean()\n",
        "    mrr = full_df[\"mrr\"].mean()\n",
        "\n",
        "    metric_df = pd.DataFrame(\n",
        "        {\"retrievers\": [name], \"hit_rate\": [hit_rate], \"mrr\": [mrr]}\n",
        "    )\n",
        "\n",
        "    return metric_df"
      ],
      "metadata": {
        "id": "mxiNl6TurpZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_results(\"top-2 eval\", eval_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "yZeIaMWKrsY1",
        "outputId": "9b8a9e7b-fdd8-43f4-9de0-4ece66c0cf05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   retrievers  hit_rate       mrr\n",
              "0  top-2 eval  0.861111  0.791667"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1b0c6382-0dd5-412c-acc9-5edc570f353e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>retrievers</th>\n",
              "      <th>hit_rate</th>\n",
              "      <th>mrr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>top-2 eval</td>\n",
              "      <td>0.861111</td>\n",
              "      <td>0.791667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b0c6382-0dd5-412c-acc9-5edc570f353e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1b0c6382-0dd5-412c-acc9-5edc570f353e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1b0c6382-0dd5-412c-acc9-5edc570f353e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HI3ffLaawktO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}